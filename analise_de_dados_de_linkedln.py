# -*- coding: utf-8 -*-
"""analise-de-dados-de-linkedln.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/leandroManjate/a976f9a4f19d62349f8e58d6a6613305/analise-de-dados-de-linkedln.ipynb

**Analise de Dados de Linkedln** *Texto em itálico*

# Analise de dados

CSV Datasheet
"""

import pandas as pd
import plotly.express as px
import numpy as np

!pip install --upgrade plotly

""" Conexão

"""

contactos = pd.read_csv('/content/Connections.csv')

contactos.shape

contactos.columns

contactos.head()

contactos.drop(labels = ['Email Address'] , axis=1 , inplace= True)

contactos.head()

contactos.isna().sum()

contactos.shape

contactos.dropna(inplace=True)

contactos.shape

contactos.isna().sum()

contactos['Full Name'] = contactos['First Name']+' '+contactos['Last Name']

contactos.head()

contactos.drop(labels = ['First Name','Last Name'],axis =1, inplace= True)
contactos

"""# Convite"""

convite = pd.read_csv('/content/Invitations.csv')

convite.columns

convite.shape

convite.head(10)

convite.isna().sum()

convite[convite['Message'].notnull()].head()

convite=convite[convite['Direction'] == 'INCOMING']

convite.shape

convite.head()

np.unique(convite['Direction'])

convite['Company']=''
convite.head()

contactos[contactos['Full Name'] == 'Daniel Silva']['Company'].values[0]

len(convite)

comvite = convite.reset_index(drop=True)
convite

for i in range (489, 489+len(convite)):
 # print(i, convite['From'][i])
 try:
  company=contactos[contactos['Full Name'] == convite['From'][i]]['Company'].values[0]
  print(company)
  convite['Company'][i]= company
 except:
    continue

convite

convite.drop(labels=['From','To','Sent At','Message','Direction'], axis=1,inplace= True)
convite

convite.to_csv('convite.csv')

"""**Geração de dados falsos**"""

contactos.columns

contactos.drop(labels = ['First Name','Last Name','Full Name'],axis =1, inplace= True)
contactos

!pip install faker

from faker import Faker

fake=Faker()

fake.name()

contactos['Full Name'] =''
contactos.head()

contactos=contactos.reset_index(drop=True)

for i in range(0, len(contactos)):
    contactos['Full Name'][i] = fake.name()

contactos

"""# **Mensagem**

"""

len(contactos)

mensagem = pd.read_csv('/content/messages.csv')

mensagem.columns



mensagem.shape

mensagem.head(1)

mensagem=mensagem['CONTENT']

mensagem

mensagem.isna().sum()

mensagem.dropna(inplace=True)
mensagem.shape

"""# Dados de conexão

# Numero de conexão por periodo
"""

!pip install --upgrade plotly

import pandas as pd
import plotly.express as px
import numpy as np
import datetime

contactos.to_csv('contactos.csv')

contactos

"""**Conexão por data**"""

contactos

def change_date(date):
  return datetime.datetime.strptime(date,'%d %b %Y').strftime('%Y-%m-%d')

contactos['Connected On'] = contactos['Connected On'].apply(change_date)

graph=px.scatter(contactos, x = 'Full Name',y='Connected On' )
graph.show()

contactos.head()

contactos.groupby(by='Connected On').count()

graph = px.line(contactos.groupby(by='Connected On').count(),title='Novas conexões por data')
graph.show()

"""**Conexão por mes**"""

def set_month(date):
  return datetime.datetime.strptime(date,'%Y-%m-%d').strftime('%m')

contactos['Connected Month'] = contactos['Connected On'].apply(set_month)
contactos

contactos.groupby(by='Connected Month').count()

graph = px.line(contactos.groupby(by='Connected Month').count(),title='Conexão por mes')
graph.show()

"""**Conexões por Ano**"""

def set_year(date):
  return datetime.datetime.strptime(date,'%Y-%m-%d').strftime('%Y')

contactos['Connected Year'] = contactos['Connected On'].apply(set_year)
contactos

contactos.groupby(by ='Connected Year').count()

graph= px.line(contactos.groupby(by='Connected Year').count(),title ='Novas conexões por ano ')
graph.show()

"""# **Empresas**"""

contactos.head()

np.unique(contactos['Company'],return_counts
=True)

contactos.shape

len(np.unique(contactos['Company']))

graph= px.histogram(contactos['Company'])
graph.show()

contactos.shape

graph=px.treemap(contactos,path=['Company'])
graph.show()

graph=px.treemap(contactos.head(1500),path=['Company','Position'])
graph.show()

graph=px.treemap(contactos.head(1500),path=['Company','Position','Full Name'])
graph.show()

"""Posição(Titulo de trabalho)"""

np.unique(contactos['Position'], return_counts=True)

contactos.shape

len(np.unique(contactos['Position']))

graph= px.histogram(contactos['Position'])
graph.show()

def mudar_posicao(position):
  nova_posicao = position
  if(position=='IT Recruiter'):
    nova_posicao= 'Talent Acquisition'
  return nova_posicao

contactos['Position'] =contactos['Position'].apply(mudar_posicao)

contactos[contactos['Position']=='IT Recruiter']

graph=px.treemap(contactos, path=['Position'])
graph.show()

"""# Levenshtein distance
---

#Rain -> shine
#rain->raine -> rhine -> shine
"""

#pip install nltk
import nltk

from nltk.metrics.distance import edit_distance

edit_distance('rain','shine')

edit_distance('IT Recruiter','Talent Acquisition')

"""**N-gram similarity**

bigrams_position1= nltk.bigrams('business intelligence analyst')
bigrams_position1

---
"""

'business intelligence analyst'.split()

bigrams_position1= list(nltk.bigrams('business intelligence engineer'.split(),pad_right=True,pad_left=True))
bigrams_position1

bigrams_position2= list(nltk.bigrams('data scientistt'.split(),pad_right=True,pad_left=True))
bigrams_position2

bigrams_position3= list(nltk.bigrams('data analyst'.split(),pad_right=True,pad_left=True))
bigrams_position3

set(['A','B','C']).intersection(set(['C','A','F']))

#Position 1 X Position 2
len(set(bigrams_position1).intersection(set(bigrams_position2)))

#Position 1 x Position 3
len(set(bigrams_position1).intersection(set(bigrams_position3)))

#Position 2 x Position 1
len(set(bigrams_position2).intersection(set(bigrams_position1)))

#Position 2 x Position 3
len(set(bigrams_position2).intersection(set(bigrams_position3)))

#Position 1 x Position 1
len(set(bigrams_position1).intersection(set(bigrams_position1)))

"""
**Jaccard distance**"""

position1 = 'business intelligence engineer'.split()
position2 = 'data scientist'.split()
position3 = 'data analyst'.split()

position1, position2, position3

#Position 2 x Position 3
intersection = set(position2).intersection(set(position3))
intersection

union = set(position2).union(set(position3))
union

len(intersection),len(union)

(len(union)-len(intersection))/len(union)

from nltk.metrics.distance import jaccard_distance

#Position 1 x Position 2
jaccard_distance(set(position1),set(position2))

#Position 1 x Position 3
jaccard_distance(set(position1),set(position3))

#Position 2 x Position 3
jaccard_distance(set(position2),set(position3))

#Position 1 x Position 1
jaccard_distance(set(position1),set(position1))

"""# Clusteringnthe position

Primeiro teste
"""

import pandas as pd

position_df = pd.DataFrame(columns=['Position'],data=['Programmer','Data Scientist','Data scientist','Business intelligence analyst', 'Data analyst','Consultant','Consultor'])

position_df

all_positions = position_df['Position'].values
all_positions

for position1 in all_positions:
  print(position1)
  print('-------------')

  for position2 in all_positions:
    print(position2,jaccard_distance(set(position1),set(position2)))
  print('\n')

min_distance= 0.5
clusters ={}
for position1 in all_positions:
   clusters[position1]=[]
   for position2 in all_positions:
     if position2 in clusters[position1] or position2 in clusters and position1 in clusters[position2]:
       continue
     distance= jaccard_distance(set(position1),set(position2))
     if distance <=min_distance:
       clusters[position1].append(position2)

clusters

clusters =[clusters[position] for position in clusters if len(clusters[position])>1]
clusters

"""# Applying in the dataset

"""

contactos.to_csv('contactos.csv')

"""Na necessidade de precisarmos de trocar os ficheiro inseridos"""

contactos = pd.read_csv('/content/connections_V1.csv')

contactos

#pip install nltk
import nltk

from nltk.metrics.distance import jaccard_distance

all_positions = contactos['Position'].values
all_positions

len(all_positions)

all_positions=set(all_positions)
all_positions
len(all_positions)

min_distance= 0.1
clusters ={}
for position1 in all_positions:
   clusters[position1]=[]
   for position2 in all_positions:
     if position2 in clusters[position1] or position2 in clusters and position1 in clusters[position2]:
       continue
     distance= jaccard_distance(set(position1),set(position2))
     if distance <=min_distance:
       clusters[position1].append(position2)

min_distance = 0.2
clusters = {}
for position1 in all_positions:
  clusters[position1] = []
  for position2 in all_positions:
    if position2 in clusters[position1] or position2 in clusters and position1 in clusters[position2]:
      continue
    distance = jaccard_distance(set(position1), set(position2))
    if distance <= min_distance:
      clusters[position1].append(position2)

cluster= [clusters[position] for position in clusters if len(clusters[position])>1]
clusters

len(clusters)

"""# Ligação ente os utilizadores"""

len(contactos)

contactos

for i in range(0,len(contactos)):
  print(contactos['Position'][i])

cluster_contacts={}
for cluster in clusters:
  print(clusters)
  cluster_contacts[tuple(cluster)]= []
  for contact in range(0,len(contactos)):
    if contactos['Position'][contact] in cluster:
      cluster_contacts[tuple(cluster)].append(contactos['Full Name'][contact])

cluster_contacts

"""# Visualização

"""

from IPython.core.display import HTML
for positions in cluster:
  #print(positions)
  string_positions = 'List of positions in the cluster: '+', '.join(positions)

  terms = set(positions[0].split())
  #print(terms)
  for words in positions:
    terms.intersection_update(set(words.split()))
  if len(terms) ==0:
    terms = ['** None term in commom **']
  terms_to_print = 'Commom terms: ' + ','.join(terms)


  print(string_positions)
  print('\n' + terms_to_print)
  print('------------------------')

  display(HTML(F'<h3>{string_positions} </h3>'))
  display(HTML(F'<p>{terms_to_print} </p>'))
  display(HTML(F'<p>{"-----------------"} </p>'))

"""# Messagem

Carregar dataset
"""

import pandas as pd
messages = pd.read_csv('/content/messages.csv')
messages

messages.drop(labels = ['CONVERSATION ID','CONVERSATION TITLE'],axis =1, inplace= True)
messages

messages.drop(labels = ['DATE','SUBJECT'],axis =1, inplace= True)
messages

messages.drop(labels = ['FROM','TO'],axis =1, inplace= True)
messages

messages.drop(labels = ['SENDER PROFILE URL','FOLDER'],axis =1, inplace= True)
messages

messages.isnull().sum

messages['CONTENT'][100]

messages.describe()

"""# Preprocessing the text

"""

from bs4 import BeautifulSoup

import re

import nltk

nltk.download('stopwords')

print(nltk.corpus.stopwords.words('portuguese'))

nltk.download('punkt')

import string
string.punctuation

def preprocessing(text):
  text = text.lower()
  text = BeautifulSoup(text,'lxml').text
  text = re.sub(r'https?://[A-Za-z0-9./]+',' ',text)
  tokens =[]
  for token in nltk.word_tokenize(text):
    tokens.append(token)

  token =[word for word in tokens if word not in nltk.corpus.stopwords.words('portuguese') and word not in string.punctuation]
  formatted_text = ''.join(element for element in tokens)

  return formatted_text

preprocessing(messages['CONTENT'][1000]+'https://www.iaexpert.academy')

"""# Preprocessing the dataset"""



messages

for i in range(0,len(messages)):
  preprocessing(messages['CONTENT'][i])

messages

messages['CONTENT'].describe()

"""# Analise de sentimentos"""

import nltk
nltk.download('vader_lexicon')

from nltk.sentiment.vader import SentimentIntensityAnalyzer

classifier = SentimentIntensityAnalyzer()
classifier.polarity_scores('I love this food')

classifier.polarity_scores('I hate this food')

result = classifier.polarity_scores('I have this food')

result

result['neg'], result['neu'], result['pos']

messages

# @title Texto de título predefinido
messages_en = messages.reset_index(drop = True)

messages_en['SENTIMENT'] = None
for i in range(0, len(messages_en)):
  sentiment = classifier.polarity_scores(messages_en['CONTENT'][i])
  #print(sentiment)
  if sentiment['pos'] > sentiment['neg'] and sentiment['pos'] > sentiment['neu']:
     messages_en['SENTIMENT'][i] = 'Positive'
  elif sentiment['neg'] > sentiment['pos'] and sentiment['neg'] > sentiment['neu']:
    messages_en['SENTIMENT'][i] = 'Negative'
  elif sentiment['neu'] > sentiment['pos'] and sentiment['neu'] > sentiment['neg']:
    messages_en['SENTIMENT'][i] = 'Neutral'
  else:
    messages_en['SENTIMENT'][i] = 'No classification'